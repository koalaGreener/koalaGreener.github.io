<h1>Weijie HUANG</h1>

<ul>
<li>Mobile：+44 07421317267 </li>
<li>Email：<a href="mailto:weijie.huang@ucl.ac.uk">weijie.huang@ucl.ac.uk</a></li>
</ul>

<hr>

<h1>Education</h1>

<ul>
<li>UCL Master Degree, Computer Science, 2015 - 2016</li>
<li>Beijing Normal University, Zhuhai Bachelor Degree, Software Engineering, 2011 - 2015 GPA 4.0/5.0</li>
<li>Fu Jen Catholic University (Taiwan) Government-Sponsored Exchange Student, Software Engineering, 2012 - 2013 GPA 4.3/5.0</li>
<li>Blog：<a href="https://koalagreener.github.io">https://koalagreener.github.io</a>.</li>
<li><p>Github：<a href="https://github.com/koalagreener">https://github.com/koalagreener</a></p></li>
<li><p>Expected position： Data Scientist, Research Engineer</p></li>
<li><p>Location：Any location</p></li>
</ul>

<hr>

<h1>Projects</h1>

<h2>University College London (Oct,2015 ~ Feb,2016）</h2>

<ul>
<li><a href="https://github.com/koalaGreener/NLP-Assignments">NLP Assignments</a>：The source code of three NLP projects and the reports.</li>
</ul>

<h3>Sentence Representations for Sentiment Analysis on Twitter</h3>

<p>In this assignment, we investigate representation learning for sentiment analysis of tweets. To this end, we build our simple deep learning framework using Recurrent Neural Network and Long Short-Term Memory respectively and learn the task-specific word and sentence representations. The best accuracy result of test dataset is 77.62%.</p>

<h3>Biomedical Event Extraction</h3>

<p>In this assignment, sentences are mapped to structured representations of the biomedical events. We use Perceptron Algorithm and Loglinear Models, which is a set of binary feature functions to predict the biomedical events. We have implemented the feature functions in several ways, such as lexical feature,entity-based feature,syntax-based feature. Comparing to Naive Bayes Algorithm(MLE), the result of our model that trained by Perceptron Algorithm is XX% higher.</p>

<h3>Language Models for processing rap corpus</h3>

<p>In this assignment, I design and implement specific language models and apply them to the provided rap corpus. By comparing different results with different language models, I chose the best pair of the parameter to reduce Perplexity from 4216 to 336, and the result of document classification peaked at 90% and 96.67% respectively on two datasets.</p>

<h2>Beijing Normal University, Zhuhai （Mar,2014 ~ May,2014）</h2>

<h3>Alibaba Big Data Competition</h3>

<ul>
<li>Judging customers’ repurchase possibility by filtering and Slope One algorithm, and outputting result based on data (such as purchase, add to favorite and add to cart) provided by Alibaba </li>
<li>Creating data filter definitions, setting up parameters and adapting algorithm to actual requirements</li>
</ul>

<hr>

<h1>Experience</h1>

<h2>BNUZ Enactus (NGO) Team, Phyt-Refine Project （ Oct,2011 ~ Jun,2014 ）</h2>

<p>-This project aims to Cultivate herbs in the rivers to cleanse polluted water and then refine the compound element from the herbs to produce profits and create job opportunities. </p>

<p>-In this three years, I mainly responsible for presenting the project during competitions, making and clipping the videos, and slides. Also, I participated in planting herbs in the watercourse and conducted an in-depth investigation in polluted rural areas and proposing practical.</p>

<ul>
<li><p>With three years of relentless efforts, Zhuhai Municipal Government moved the biggest polluting source in 2013 and invested in planting herbs at the mouth of Pearl River. As a result, water quality has been remarkably improved.</p></li>
<li><p>Winning Top 10 Team (221 teams in total) Award and the Project Incubation Opportunity in 2013 Junior Achievement YESxBOP Business Plan Competition</p></li>
<li><p>Winning National 2nd Prize (227 universities in total) in 2014 Enactus China National Competition</p></li>
</ul>

<hr>

<h1>Honors and Awards</h1>

<p>Beijing Normal University, Zhuhai 
- Top Ten Graduates, 2015
- First Prize Scholarship, three times, 2012-2014</p>

<hr>

<h1>Greeting</h1>

<p>I hope that we can work together in the future, and thank you for reading my resume.</p>
